{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pDL] *",
      "language": "python",
      "name": "conda-env-pDL-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Extending PyTorch differentiable functions.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAASDwD1kFjG",
        "colab_type": "text"
      },
      "source": [
        "# Extending PyTorch differentiable functions\n",
        "\n",
        "In this notebook you'll see how to add your custom differentiable function for which you need to specify `forward` and `backward` passes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD9xAx9fkdTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "18e3be72-745a-447f-af92-674550e13abd"
      },
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.0.post4\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl (592.3MB)\n",
            "\u001b[K     |████████████████████████████████| 592.3MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==0.3.0.post4) (1.18.5)\n",
            "\u001b[31mERROR: torchvision 0.7.0+cu101 has requirement torch==1.6.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement torch>=1.0.0, but you'll have torch 0.3.0.post4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.6.0+cu101\n",
            "    Uninstalling torch-1.6.0+cu101:\n",
            "      Successfully uninstalled torch-1.6.0+cu101\n",
            "Successfully installed torch-0.3.0.post4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWHVIXngkqqK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "ad1d33bc-5b5c-47d7-ab8d-491fffe876d9"
      },
      "source": [
        "!pip3 install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Collecting torch==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 0.3.0.post4\n",
            "    Uninstalling torch-0.3.0.post4:\n",
            "      Successfully uninstalled torch-0.3.0.post4\n",
            "Successfully installed torch-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxD_RfUlkFjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import some libraries\n",
        "import torch\n",
        "import numpy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO9vicbdkFjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom addition module\n",
        "class MyAdd(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x1, x2):\n",
        "        # ctx is a context where we can save\n",
        "        # computations for backward.\n",
        "        ctx.save_for_backward(x1, x2)\n",
        "        return x1 + x2\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x1, x2 = ctx.saved_tensors\n",
        "        grad_x1 = grad_output * torch.ones_like(x1)\n",
        "        grad_x2 = grad_output * torch.ones_like(x2)\n",
        "        # need to return grads in order \n",
        "        # of inputs to forward (excluding ctx)\n",
        "        return grad_x1, grad_x2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeFJP4QNkFjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "dc32a9df-1e96-4479-83ac-cfbe6ed05724"
      },
      "source": [
        "# Let's try out the addition module\n",
        "x1 = torch.randn((3), requires_grad=True)\n",
        "x2 = torch.randn((3), requires_grad=True)\n",
        "print(f'x1: {x1}')\n",
        "print(f'x2: {x2}')\n",
        "myadd = MyAdd.apply  # aliasing the apply method\n",
        "y = myadd(x1, x2)\n",
        "print(f' y: {y}')\n",
        "z = y.mean()\n",
        "print(f' z: {z}, z.grad_fn: {z.grad_fn}')\n",
        "z.backward()\n",
        "print(f'x1.grad: {x1.grad}')\n",
        "print(f'x2.grad: {x2.grad}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x1: tensor([ 1.0212, -1.1056,  0.4566], requires_grad=True)\n",
            "x2: tensor([0.1209, 1.1487, 0.9463], requires_grad=True)\n",
            " y: tensor([1.1421, 0.0431, 1.4029], grad_fn=<MyAddBackward>)\n",
            " z: 0.8626823425292969, z.grad_fn: <MeanBackward0 object at 0x7ff71288a828>\n",
            "x1.grad: tensor([0.3333, 0.3333, 0.3333])\n",
            "x2.grad: tensor([0.3333, 0.3333, 0.3333])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5rWo9_jkFjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom split module\n",
        "class MySplit(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        x1 = x.clone()\n",
        "        x2 = x.clone()\n",
        "        return x1, x2\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_x1, grad_x2):\n",
        "        x = ctx.saved_tensors[0]\n",
        "        print(f'grad_x1: {grad_x1}')\n",
        "        print(f'grad_x2: {grad_x2}')\n",
        "        return grad_x1 + grad_x2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWqFxJPFkFjY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "aa3170fa-7aa9-4d17-cb80-24bc2a360bf9"
      },
      "source": [
        "# Let's try out the split module\n",
        "x = torch.randn((4), requires_grad=True)\n",
        "print(f' x: {x}')\n",
        "split = MySplit.apply\n",
        "x1, x2 = split(x)\n",
        "print(f'x1: {x1}')\n",
        "print(f'x2: {x2}')\n",
        "y = x1 + x2\n",
        "print(f' y: {y}')\n",
        "z = y.mean()\n",
        "print(f' z: {z}, z.grad_fn: {z.grad_fn}')\n",
        "z.backward()\n",
        "print(f' x.grad: {x.grad}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " x: tensor([-0.0619, -0.1381, -0.1914, -0.4372], requires_grad=True)\n",
            "x1: tensor([-0.0619, -0.1381, -0.1914, -0.4372], grad_fn=<MySplitBackward>)\n",
            "x2: tensor([-0.0619, -0.1381, -0.1914, -0.4372], grad_fn=<MySplitBackward>)\n",
            " y: tensor([-0.1238, -0.2762, -0.3828, -0.8744], grad_fn=<AddBackward0>)\n",
            " z: -0.4143010079860687, z.grad_fn: <MeanBackward0 object at 0x7ff712894908>\n",
            "grad_x1: tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            "grad_x2: tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
            " x.grad: tensor([0.5000, 0.5000, 0.5000, 0.5000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyCEzLLxkFjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom max module\n",
        "class MyMax(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        # example where we explicitly use non-torch code\n",
        "        maximum = x.detach().numpy().max()\n",
        "        argmax = x.detach().eq(maximum).float()\n",
        "        ctx.save_for_backward(argmax)\n",
        "        return torch.tensor(maximum)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        argmax = ctx.saved_tensors[0]\n",
        "        return grad_output * argmax"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QbaIs-LykFje",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "18f943ea-d131-4fbc-fb70-b677e51cfff9"
      },
      "source": [
        "# Let's try out the max module\n",
        "x = torch.randn((5), requires_grad=True)\n",
        "print(f'x: {x}')\n",
        "mymax = MyMax.apply\n",
        "y = mymax(x)\n",
        "print(f'y: {y}, y.grad_fn: {y.grad_fn}')\n",
        "y.backward()\n",
        "print(f'x.grad: {x.grad}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: tensor([ 0.1203, -0.9562,  1.1946, -0.5337,  0.0887], requires_grad=True)\n",
            "y: 1.1945806741714478, y.grad_fn: <torch.autograd.function.MyMaxBackward object at 0x7ff7128df668>\n",
            "x.grad: tensor([0., 0., 1., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}