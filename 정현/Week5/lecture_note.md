# Optimisation Techniques

## Gradient Descent

![G_d](https://blog.paperspace.com/content/images/2018/05/sgd.png)   
__find the lowest point == valley__   
+ function L: continuous, differentiable_   
+ Î³: __step size == learning rate__, a little larger than the optimal to converge   
![lr](https://atcold.github.io/pytorch-Deep-Learning/images/week05/05-1/step-size.png)   


## Stochastic Gradient Descent

