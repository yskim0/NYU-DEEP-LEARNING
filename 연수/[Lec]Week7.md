# NYU Deep Learning - Week 7

## Feed-Forward Nets의 문제점

1. inference 과정이 여러 레이어의 가중 합보다 더 크고 복잡한 계산이 된다면?
2. 하나의 입력값이 여러 개의 출력값을 가질 수 있게 된다면? (ex. 비디오의 미래 프레임 예측)

## Energy-Based Model 

- 에너지 기반 모델은 x에서 y를 분류하는 것보다, 특정 (x,y)의 짝들이 서로 맞는지 아닌지를 예측함.
    - 즉, **x에 양립하는 y를 찾음.**


**F(x,y)가 낮은 y를 찾으며 우리는 문제를 제기할 수 있음**
- y는 x의 정확한 고해상도 이미지인가?
- 문서 A는 문서 B의 좋은 번역문인가?

### Definition

F : X * Y -> R이며, `F(x,y)`가 (x,y)의 dependency를 설명함.

이 에너지는 추론에 사용되지만 **학습에는 사용되지 않음.**

- 추론 : y_hat = argmin_y{F(x,y)}

### Gradient-based inference

에너지 함수는 smooth하고 미분 가능한 함수여야 경사도-기반 방법을 추론에 사용할 수 있다.

추론을 하기 위해서, 이 함수를 gradient descent를 이용해 양립적인 y를 찾는다.

## Latent Variable을 가진 Energy-based Model

- 출력값 y는 x와 우리가 값을 모르는 추가 변수 z(잠재변수)에 의해 결정된다.
- 잠재변수는 보조정보(auxilliary information)을 제공한다.
    - 잠재변수는 글뭉치 사이사이 각 단어들의 경계를 알려줄 수도 있고, 띄어쓰기 없는 손글씨 해석에도 도움을 줄 수 있다.

### Inference

- 잠재변수를 가진 에너지 기반 모델을 추론하기 위해서는 에너지 함수를 y와 z에 대해서 최소화 해야한다.
    - y_hat, z_hat = argmin_{y,z} E(x,y,z)

- 재정의

![image](https://user-images.githubusercontent.com/48315997/91940718-e4250300-ed32-11ea-84bf-2a39d5e9aedc.png)

- 잠재변수를 허용하는 또 다른 큰 장점은 잠재변수를 변화시킴으로써 예측 출력값 y를 가능한 예측값 manifold에 따라 변화 가능하게 만드는 것이다. (아래 사진의 리본모양)
    - 이는 기계가 단 한 가지의 결과값이 아닌 여러가지의 출력값을 갖는 것을 가능케한다.

<img src="https://atcold.github.io/pytorch-Deep-Learning/images/week07/07-1/fig1.png" width=500>

## 에너지 기반 모델 v.s. 확률 모델

- 에너지는 비정규화된 음수 로그 확률로 볼 수 있고, 정규화 이후 Gibbs-Boltzmann distribution을 사용하여 확률로 변환할 수 있다.

<img width="300" alt="스크린샷 2020-09-02 오후 3 47 34" src="https://user-images.githubusercontent.com/48315997/91941107-a07ec900-ed33-11ea-9ad8-16ca6dab0a77.png">

- beta는 positive constant이며 모델을 적합하기 위해 조정되어야 함.
    - beta가 클수록 모델 변동은 더욱 심해지고, 작을수록 모델은 매끄러워진다.
    - 물리학에서는 역온도를 뜻한다. beta가 무한대로가면 온도가 0으로 수렴한다는 뜻.

- 만약 y에 주변화(marginalize)를 하면 다음과 같은 수식을 갖게 된다.

<img width="700" alt="스크린샷 2020-09-02 오후 3 49 31" src="https://user-images.githubusercontent.com/48315997/91941266-e6d42800-ed33-11ea-92b9-d1970f21659d.png">


### 자유 에너지


- 자유 에너지 재정립함으로써 잠재 변수 모델에서의 잠재변수 z를 확률적으로 올바르게 제거 가능하다.

<img width="700" alt="스크린샷 2020-09-02 오후 3 51 12" src="https://user-images.githubusercontent.com/48315997/91941407-23a01f00-ed34-11ea-9879-13c498b299eb.png">


- 위 수식을 계산하는 건 굉장히 어렵고, 실제로는 불가능에 가깝다.

-  만약 모델 안에 최소화하고 싶은 잠재변수를 가졌거나, 주변화를 하고 위 공식의 무한 \betaβ극값에 대해 최소화 하려는 잠재변수를 갖는다면, 계산이 가능하기는 하다.

### 에너지 기반 모델과 확률 기반 모델의 차이점

- 확률 모델 : 최소화할 objective function을 직접 고를 수 없고, 우리가 다루는 모든 항목들이 정규된 분포여야한다는 확률적 구조에 얽매여야 한다는 것
    - variational methods 등으로 근사시킬 수는 있다.
    - > 여기서, 이 모델들을 통해 우리가 하고자 하는 것은 결국 의사결정을 내는 것이다. 자동차를 운전시키는 시스템을 만들었다고 가정하고, 그 시스템이 0.8의 확률로 좌회전을 하라고 하고 0.2의 확률로 우회전을 하라고 한다고 해보자. 확률이 0.2든 0.8이라는 것은 전혀 중요치 않다 – 의사결정을 내려야만 하는 우리가 정말 원하는 것은 가장 최적의 의사결정을 만드는 것이다. **그러므로 의사결정을 내려야하는 상황에서 확률은 전혀 쓸모가 없다.**

- 에너지 기반 모델 : 우리가 어떻게 모델을 다룰지, 어떤식으로 학습을 시킬지, 어떤 목적 함수를 사용할지에 대해 더 많은 선택지를 제공한다.


>  만약 확률적 모델을 고수한다면, 최대우도법maximum likelihood을 사용해야만 한다 – 관찰된 데이터에서의 확률이 최대가 되도록 모델을 학습시켜야한다. 문제는 이러한 방식들은 우리가 사용하는 모델들이 “옳은” 모델이여야 하는 경우에만 사용이 가능하다는 것이다 – 그리고 모델은 절대로 “옳을” 수 없다. 저명한 통계학자인 [Goerge Box]는 이렇게 말하였다, “모든 모델들은 전부 틀렸다. 하지만 그 중 일부는 쓸만하다” 그러므로 특히 고차원 공간에서나 문서들과 같은 조합 공간에서의 확률 모델들은 전부 근사모델이다. 어느 방면으로든 확률 모델은 전부 틀렸고, 저 틀린 모델들을 정규화 시키려한다면, 문제를 더 틀리게 만드는 것이다. 그래서 정규화를 시키지 않는 것이 더 나은 방법이다.


----

## Self Supervised Learning

- Self Supervised Learning은 지도학습과 비지도학습 모두를 포함한다.
    - 자기지도 학습 모델은 데이터의 일부를 바탕으로 그 나머지를 예측하도록 훈련된다.
    - BERT는 이 학습 기법을 이용해 학습되었고, Denoising Auto-Encoder는 NLP 부문에서 성과를 이루고 있다.

![1](https://atcold.github.io/pytorch-Deep-Learning/images/week07/07-2/1_ssl.png)

- 자기지도 학습의 과제
    - 과거로부터 미래를 예측하는 것
    - 보이는 것을 바탕으로 보이지 않는 것(the masked)을 예측하는 것
    - 가용할 수 있는 모든 부분을 바탕으로 가려진 부분을 예측하는 것

- 컴퓨터 비전 부문에서 위와 같은 방식으로 학습된 모델이 누락 된 공간을 채울 수는 있었지만, NLP 시스템과 같은 수준의 성과를 이뤄내지는 못했다.

- 이 지점에서 NLP와 컴퓨터 비전 시스템 사이의 차이는 **NLP는 이산적 discrete인 반면, 이미지는 연속적continuous이라는 것이다.** 
    - 이산 영역에서 우리는 불확실성을 표현하는 방법을 알고 있고, 가능한 출력에 대해 큰 softmax값을 사용할 수 있지만, 연속적 영역에서는 그렇게 할 수 없기 때문에 각 시스템의 성과에 차이가 있었다.

- 우리 세계는 완전히 결정론적이지 않고, 모든 가능성을 설명할 수는 없기 때문에 AI 시스템에게 고차원 공간 속의 불확실성 아래에서도 예측을 할 수 있도록 훈련시켜야 한다.
    - `Energy-based Model`이 유용할 수 있다.


### 다음 프레임 예측을 위한 잠재 변수 에너지 기반 모델

- 선형 회귀 분석과 다르게 잠재 변수 에너지 기반 모델은 우리가 세계에 대해 알고 있는 것 뿐만 아니라 실제에서 일어난 일들에 대한 정보를 제공하는 잠재변수를 사용한다.

- 이 두 가지 정보 조합은 실제로 발생하는 것들에 가까운 예측을 해내는데 사용할 될 수 있다.

- 시스템 에너지를 최소화하는 잠재 변수를 이용한 예측을 바탕으로 입력 x와 실제 출력 y사이의 호환성을 평가하는 시스템
    - 입력 x를 관찰하고, 이 입력 x와 잠재 변수들 z과의 다양한 조합을 이용해 예측값 y를 생성하여 시스템의 에너지, 예측 오류를 최소화하는 것을 선택한다.

- 뽑아내는 잠재 변수에 따라 가능한 모든 예측값을 얻을 수 있다. 잠재 변수는 입력 x에 없는 출력 y와 관련된 중요한 정보로 간주할 수 있다.

- scalar-value 에너지 함수는 두 가지 버전을 가진다.
    - conditional F(x,y) : x와 y사이의 호환성 측정
    - unconditional F(y) : y의 구성 요소 사이의 호환성 측정

## 에너지 기반 모델 학습시키기

F(x,y)를 매개변수화 하기 위한 에너지 기반 모델을 훈련시키는 두 개의 학습 모델 클래스가 있다.

1. **대조적(Contrastive)방법** : F(x[i],y[i])을 밀어 내리고, 다른 점들 F(x[i],y')을 밀어 올리는 것
2. **구조적(Architectural)방법** : 낮은 에너지 영역의 크기가 정규화를 통해 제한되거나 최소화되는 F(x,y)를 만들어내는 것

- 대조적 방법의 예시는 **최대 우도 학습Maximum Likelihood learning이다**. 
    - 에너지는 표준화되지 않은 음의 로그 밀도 함수로 간주될 수 있다. 
    - 깁스 분포Gibbs distribution 는 x 가 주어진 상태에서 y 의 우도likelihood 를 제공한다. 
    - 최대 우도는 분자를 크게 하고, 분모를 작게 해서 우도 값을 최대화 하고자 한다.

    <img width="800" alt="스크린샷 2020-09-02 오후 4 25 37" src="https://user-images.githubusercontent.com/48315997/91944381-f144f080-ed38-11ea-9568-a708db5d7968.png">

    - 음의 우도 손실의 경사값은 다음과 같다.

    <img width="800" alt="스크린샷 2020-09-02 오후 4 26 29" src="https://user-images.githubusercontent.com/48315997/91944465-10dc1900-ed39-11ea-9aed-fd5b589baefc.png">


## 잠재 변수 에너지 기반 모델

잠재변수 모델의 주요 장점은 잠재변수를 이용한 다양한 예측이 가능하다는 것.
- z가 어떤 집합 안에서 변함에 따라, y는 가능한 예측값을 원소로 하는 매니폴드에서 변화한다.

![스크린샷 2020-09-02 오후 4 28 19](https://user-images.githubusercontent.com/48315997/91944613-51d42d80-ed39-11ea-9b5d-9264242a2aa0.png)


### EBM 예시 : K-means

y의 분포를 모델링 하려는 에너지 기반 모델로 간주될 수 있다.

![2](https://atcold.github.io/pytorch-Deep-Learning/images/week07/07-2/4_kmeans.png)

- 에너지함수 E(y,z) = ||y-Wz||^2

- y와 k값이 주어지면 W의 어떤 k개 가능한 열들이 reconstruction 오류, 혹은 에너지 함수를 최소화하는지 파악하여 추론할 수 있다.


알고리즘 훈련을 위해서, z를 찾아서 y와 가장 가까운 W의 열을 선택하고 여기서 경사하강법을 수행하고, 이 과정을 반복하여 목표하는 최솟값으로 더 가까이 접근하는 방식을 채택할 수 있다.
- 좌표 경사 하강법은 실제로 더 빠르고 효과적으로 작동한다.



에너지 함수를 알게 되면 다음과 같은 질문에도 대답을 할 수 있게 된다.
- y1의 점이 주어졌을 때, y2를 예측해낼 수 있는가?
- y가 주어졌을 때, 이 데이터 manifold와 가장 가까운 점을 찾아낼 수 있는가?


`K-means`는 구조적 방법에 속한다. 따라서 에너지를 아무데나 밀어 올리지 않고 특정 지역의 에너지를 낮춘다.

<br>

단점
- k 값이 결정되면, 에너지 값이 0인 데이터 포인트가 단 k로 한정된다는 것과, 다른 모든 포인트의 에너지 값은 그 데이터 포인트에서 멀어질수록 이차적으로 증가하는 높은 값을 갖는다는 점이다.


### 대조적 방법

 모든 사람이 언젠가는 구조적 방법을 사용할 것이지만, 현 시점에서는 이미지 데이터에 대해 잘 동작하는 것은 대조적 방법이다

 ![3](https://atcold.github.io/pytorch-Deep-Learning/images/week07/07-2/6_contrastive_1.png)

 - 이상적으로, 에너지 표현이 데이터 매니폴드 위에서 최소의 에너지를 갖기 바란다.
 - 주변의 에너지 값 (즉, F(x,y)의 값) 을 낮추려고 하지만 , 이것 만으로는 충분하지 않을 수 있다. 따라서 높은 에너지값을 가져야 하는데 낮은 에너지값을 가지고 있는 부분의 y 값을 올린다.


에너지를 높이고자 하는 y후보들을 찾는 몇 가지 방법

- Denoising Autoencoder
- Contrastive Divergence
- Monte Carlo
- Markov Chain Monte Carlo
- Hamiltonian Monte Carlo

#### Denoising Autoencoder (DAE)

y를 찾는 방법 중 하나는 아래 그림에서 녹색 화살표로 표시된 것과 같이 훈련 예제를 무작위로 교란시켜 에너지를 높이는 것이다.

![5](https://atcold.github.io/pytorch-Deep-Learning/images/week07/07-2/7_contrastive_2.png)

- 데이터 포인트가 손상되면(corrupted), 에너지를 올릴 수 있다.
- 아러한 작업을 모든 데이터 포인트에 대해서 충분히 많은 횟수로 실행하면, 에너지 샘플이 훈련 예제를 중심으로 모인다.

![6](https://atcold.github.io/pytorch-Deep-Learning/images/week07/07-2/8_training.png)

1. y 점을 하나 잡고 이것을 손상시킴
2. 이렇게 손상된 데이터 포인트에서 원본 데이터를 재구성해 낼 수 있도록 Encoder와 Decoder를 훈련시킴 
3. 만일 DAE 이 올바르게 훈련 되면, 데이터 매니폴드에서 멀어짐에 따라 에너지 값이 2차적으로(quadraticallly) 증가한다.


#### BERT

- 텍스트를 이산적인 공간에서 처리하였다는 점을 제외하면 비슷한 방식으로 훈련됨
- 손상 기법 : 일부 단어 masking
- 재구성 단계에서 이를 예측
- masked autoencoder라고도 부름

#### Contrastive Divergence

- 에너지를 끌어올리고자하는 y를 더 똑똑하게 찾는 방법 제시

- 훈련 지점에 임의의 kick을 주고 경사 하강법을 이용해 에너지 함수를 낮춤. 궤도의 끝에서 찾기하는 지점의 에너지 값을 증가시킴.
    - 아래 그림의 녹색선

![7](https://atcold.github.io/pytorch-Deep-Learning/images/week07/07-2/10_contrastive_div.png

